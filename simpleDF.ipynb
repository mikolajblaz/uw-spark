{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "sqlContext.sql(\"set spark.sql.shuffle.partitions=4\")\n",
    "sqlContext.sql(\"set spark.default.parallelism=4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "file = \"data/test-graph.txt\"\n",
    "max_iter = 100\n",
    "num_partition = 4\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lines: 16\n",
      "Correct lines: 11\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "textFile = sc.textFile(file)\n",
    "print(\"All lines:\", textFile.count())\n",
    "dataFile = textFile.filter(lambda l: len(l) > 0 and l[0] != '#')\n",
    "print(\"Correct lines:\", dataFile.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create edges RDD\n",
    "def to_int_tuple(line, delim='\\t'):\n",
    "    strings = line.split(delim)[:2]\n",
    "    return (int(strings[0]), int(strings[1]))\n",
    "    \n",
    "edgesRDD = dataFile.map(to_int_tuple)\n",
    "edgesRDD.partitionBy(num_partition)\n",
    "edgesRDD.cache()\n",
    "\n",
    "# Check that all rows have exactly 2 entries\n",
    "assert edgesRDD.filter(lambda t: len(t) == 2).count() == dataFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fro: bigint, to: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = edgesRDD.toDF([\"fro\", \"to\"])\n",
    "edges.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compose(df1, df2):\n",
    "    \"\"\" Compose 2 relations represented by PairRDDs. \"\"\"\n",
    "    r1 = df1.toDF('fro', 'inter')\n",
    "    r2 = df2.toDF('inter', 'to')\n",
    "    return r1.join(r2, 'inter').drop('inter').coalesce(num_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# RDD: method 1 (single steps) ###############\n",
      "NUM e: 2\n",
      "NUM n: 2\n",
      "________________________________\n",
      "Iteration #1:\n",
      "NUM n: 4\n",
      "Number of new paths: 13\n",
      "\n",
      "NUM a: 2\n",
      "[Row(fro=1, to=3), Row(fro=5, to=9), Row(fro=9, to=6), Row(fro=10, to=9), Row(fro=3, to=5), Row(fro=4, to=6), Row(fro=6, to=10), Row(fro=5, to=7), Row(fro=10, to=7), Row(fro=2, to=4), Row(fro=5, to=8), Row(fro=10, to=8), Row(fro=0, to=2)] \n",
      "\n",
      "Iteration time: 1.448267 s.\n",
      "________________________________\n",
      "Iteration #2:\n",
      "NUM n: 4\n",
      "Number of new paths: 13\n",
      "\n",
      "NUM a: 4\n",
      "[Row(fro=4, to=7), Row(fro=9, to=7), Row(fro=9, to=9), Row(fro=0, to=3), Row(fro=2, to=5), Row(fro=4, to=8), Row(fro=6, to=6), Row(fro=1, to=4), Row(fro=3, to=6), Row(fro=5, to=10), Row(fro=9, to=8), Row(fro=4, to=9), Row(fro=10, to=10)] \n",
      "\n",
      "Iteration time: 1.629313 s.\n",
      "________________________________\n",
      "Iteration #3:\n",
      "NUM n: 4\n",
      "Number of new paths: 7\n",
      "\n",
      "NUM a: 4\n",
      "[Row(fro=3, to=9), Row(fro=0, to=4), Row(fro=2, to=6), Row(fro=3, to=8), Row(fro=1, to=5), Row(fro=3, to=7), Row(fro=4, to=10)] \n",
      "\n",
      "Iteration time: 1.784078 s.\n",
      "________________________________\n",
      "Iteration #4:\n",
      "NUM n: 4\n",
      "Number of new paths: 6\n",
      "\n",
      "NUM a: 4\n",
      "[Row(fro=2, to=9), Row(fro=1, to=6), Row(fro=3, to=10), Row(fro=0, to=5), Row(fro=2, to=8), Row(fro=2, to=7)] \n",
      "\n",
      "Iteration time: 2.218882 s.\n",
      "________________________________\n",
      "Iteration #5:\n",
      "NUM n: 4\n",
      "Number of new paths: 5\n",
      "\n",
      "NUM a: 4\n",
      "[Row(fro=1, to=9), Row(fro=2, to=10), Row(fro=1, to=8), Row(fro=0, to=6), Row(fro=1, to=7)] \n",
      "\n",
      "Iteration time: 2.621006 s.\n",
      "________________________________\n",
      "Iteration #6:\n",
      "NUM n: 4\n",
      "Number of new paths: 4\n",
      "\n",
      "NUM a: 4\n",
      "[Row(fro=0, to=7), Row(fro=0, to=8), Row(fro=0, to=9), Row(fro=1, to=10)] \n",
      "\n",
      "Iteration time: 4.926723 s.\n",
      "________________________________\n",
      "Iteration #7:\n",
      "NUM n: 4\n",
      "Number of new paths: 1\n",
      "\n",
      "NUM a: 4\n",
      "[Row(fro=0, to=10)] \n",
      "\n",
      "Iteration time: 13.384797 s.\n",
      "________________________________\n",
      "Iteration #8:\n",
      "NUM n: 4\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2207.count.\n: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c6f68f32597d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnew_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_paths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_partition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mnew_paths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of new paths: %d\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_paths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Add new paths to all paths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mikib/app/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mikib/app/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mikib/app/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mikib/app/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2207.count.\n: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "print(\"############# RDD: method 1 (single steps) ###############\")\n",
    "new_paths = edges\n",
    "all_paths = edges\n",
    "\n",
    "start = time.time()\n",
    "true_start = start\n",
    "\n",
    "print(\"NUM e:\", edges.rdd.getNumPartitions())\n",
    "print(\"NUM n:\", new_paths.rdd.getNumPartitions())\n",
    "\n",
    "# invariant:\n",
    "###  - all_paths and new_paths are on 'num_partitions' partitions\n",
    "\n",
    "for i in range(1, max_iter):\n",
    "    print(\"________________________________\")\n",
    "    print(\"Iteration #%d:\" % (i,))\n",
    "    new_paths = compose(new_paths, edges)\n",
    "    # Leave only really new paths\n",
    "    print(\"NUM n:\", new_paths.rdd.getNumPartitions())\n",
    "    new_paths = new_paths.subtract(all_paths).distinct().coalesce(num_partition)\n",
    "    new_paths.cache()\n",
    "    print(\"Number of new paths: %d\\n\" % (new_paths.count(),))\n",
    "    \n",
    "    # Add new paths to all paths\n",
    "    print(\"NUM a:\", all_paths.rdd.getNumPartitions())\n",
    "    all_paths = all_paths.unionAll(new_paths).coalesce(num_partition)\n",
    "    all_paths.cache()\n",
    "    \n",
    "    if debug:\n",
    "        print(new_paths.take(1000), '\\n')\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Iteration time: %f s.\" % (end - start,))\n",
    "    start = end\n",
    "    \n",
    "    # Finish, when no more paths added\n",
    "    if new_paths.rdd.isEmpty():\n",
    "        print(\"No new paths, finishing...\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\\n________________________________\")\n",
    "print(\"Total paths found: %d\" % (all_paths.count(),))\n",
    "print(\"Number of iterations: #%d\" % (i,))\n",
    "\n",
    "if debug:\n",
    "    print()\n",
    "    print(all_paths.take(1000), '\\n')\n",
    "\n",
    "true_end = time.time()\n",
    "method1_time = true_end - true_start\n",
    "print(\"\\nCollecting time: %f s.\" % (true_end - start,))\n",
    "print(\"Total time elapsed: %f s.\" % (method1_time,))\n",
    "print(\"________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"############# RDD: method 2 (paths combining) ###############\")\n",
    "new_paths = edges\n",
    "all_paths = edges\n",
    "\n",
    "start = time.time()\n",
    "true_start = start\n",
    "\n",
    "# invariant:\n",
    "###  - all_paths and new_paths are on 'num_partitions' partitions\n",
    "\n",
    "for i in range(1, max_iter):\n",
    "    print(\"________________________________\")\n",
    "    print(\"Iteration #%d:\" % (i,))\n",
    "    new_paths = compose(all_paths, all_paths)\n",
    "    # Leave only really new paths\n",
    "    new_paths = new_paths.subtract(all_paths).distinct().coalesce(num_partition)\n",
    "    new_paths.cache()\n",
    "    print(\"Number of new paths: %d\\n\" % (new_paths.count(),))\n",
    "    \n",
    "    # Finish, when no more paths added\n",
    "    if new_paths.rdd.isEmpty():\n",
    "        print(\"No new paths, finishing...\")\n",
    "        break\n",
    "    \n",
    "    # Add new paths to all paths\n",
    "    all_paths = all_paths.unionAll(new_paths).coalesce(num_partition)\n",
    "    all_paths.cache()\n",
    "    \n",
    "    if debug:\n",
    "        print(new_paths.take(1000), '\\n')\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Iteration time: %f s.\" % (end - start,))\n",
    "    start = end\n",
    "\n",
    "print(\"\\n\\n________________________________\")\n",
    "print(\"Total paths found: %d\" % (all_paths.count(),))\n",
    "print(\"Number of iterations: #%d\" % (i,))\n",
    "\n",
    "if debug:\n",
    "    print()\n",
    "    print(all_paths.take(1000), '\\n')\n",
    "\n",
    "true_end = time.time()\n",
    "method2_time = true_end - true_start\n",
    "print(\"\\nCollecting time: %f s.\" % (true_end - start,))\n",
    "print(\"Total time elapsed: %f s.\" % (method2_time,))\n",
    "print(\"________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"############# RDD: method 3 (paths combining 'optimized') ###############\")\n",
    "\n",
    "start = time.time()\n",
    "true_start = start\n",
    "\n",
    "all_paths = edges\n",
    "new_paths = compose(edges, edges)\n",
    "new_paths = new_paths.subtract(all_paths).distinct().coalesce(num_partition)\n",
    "# invariant:\n",
    "###  - all_paths and new_paths are disjoint\n",
    "###  - all_paths and new_paths are on 'num_partitions' partitions\n",
    "\n",
    "for i in range(2, max_iter):\n",
    "    print(\"________________________________\")\n",
    "    print(\"Iteration #%d:\" % (i,))\n",
    "    # Obtain new paths by composing old ones\n",
    "    all_x_new_paths = compose(all_paths, new_paths)\n",
    "    new_x_all_paths = compose(new_paths, all_paths)\n",
    "    new_x_new_paths = compose(new_paths, new_paths)\n",
    "    # Leave only really new paths\n",
    "    all_paths = all_paths.unionAll(new_paths).coalesce(num_partition)\n",
    "    all_paths.cache()\n",
    "    new_paths = all_x_new_paths.unionAll(new_x_all_paths).unionAll(new_x_new_paths)\n",
    "    new_paths = new_paths.subtract(all_paths).distinct().coalesce(num_partition)\n",
    "    new_paths.cache()\n",
    "    print(\"Number of new paths: %d\\n\" % (new_paths.count(),))\n",
    "    \n",
    "    # Finish, when no more paths added\n",
    "    if new_paths.rdd.isEmpty():\n",
    "        print(\"No new paths, finishing...\")\n",
    "        break\n",
    "    \n",
    "    if debug:\n",
    "        print(new_paths.take(1000), '\\n')\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Iteration time: %f s.\" % (end - start,))\n",
    "    start = end\n",
    "\n",
    "print(\"\\n\\n________________________________\")\n",
    "print(\"Total paths found: %d\" % (all_paths.count(),))\n",
    "print(\"Number of iterations: #%d\" % (i,))\n",
    "\n",
    "if debug:\n",
    "    print()\n",
    "    print(all_paths.take(1000), '\\n')\n",
    "\n",
    "true_end = time.time()\n",
    "method3_time = true_end - true_start\n",
    "print(\"\\nCollecting time: %f s.\" % (true_end - start,))\n",
    "print(\"Total time elapsed: %f s.\" % (method3_time,))\n",
    "print(\"________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"########### Summary ############\")\n",
    "print(\"Method 1: %f s.\" % (method1_time,))\n",
    "print(\"Method 2: %f s.\" % (method2_time,))\n",
    "print(\"Method 3: %f s.\" % (method3_time,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
